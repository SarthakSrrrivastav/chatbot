{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649b42ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "    import nltk\n",
    "    from nltk.stem import PorterStemmer\n",
    "    from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89171cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8afa56e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b42a5014",
   "metadata": {},
   "outputs": [],
   "source": [
    "par=\"\"\"With a different system in place, NLP slowly improved moving from a cumbersome-rule based to a pattern learning based computer programming methodology. Siri appeared on the iPhone in 2011. In 2012, the new discovery of use of graphical processing units (GPU) improved digital neural networks and NLP.\n",
    "\n",
    "NLP empowers computer programs to comprehend unstructured content by utilizing AI and machine learning to make derivations and give context to language, similarly as human brains do. It is a device for revealing and analysing the “signals” covered in unstructured information. Organizations would then be able to get a deeper comprehension of public perception around their products, services and brand, just as those of their rivals.\n",
    "\n",
    "Now Google has released its own neural-net-based engine for eight language pairs, closing much of the quality gap between its old system and a human translator and fuelling increasing interest in the technology. Computers today can already produce an eerie echo of human language if fed with the appropriate material.\n",
    "\n",
    "Over the past few years, Deep Learning (DL) architectures and algorithms have made impressive advances in fields such as image recognition and speech processing.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77065d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen=nltk.word_tokenize(par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1e2e9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['With',\n",
       " 'a',\n",
       " 'different',\n",
       " 'system',\n",
       " 'in',\n",
       " 'place',\n",
       " ',',\n",
       " 'NLP',\n",
       " 'slowly',\n",
       " 'improved',\n",
       " 'moving',\n",
       " 'from',\n",
       " 'a',\n",
       " 'cumbersome-rule',\n",
       " 'based',\n",
       " 'to',\n",
       " 'a',\n",
       " 'pattern',\n",
       " 'learning',\n",
       " 'based',\n",
       " 'computer',\n",
       " 'programming',\n",
       " 'methodology',\n",
       " '.',\n",
       " 'Siri',\n",
       " 'appeared',\n",
       " 'on',\n",
       " 'the',\n",
       " 'iPhone',\n",
       " 'in',\n",
       " '2011',\n",
       " '.',\n",
       " 'In',\n",
       " '2012',\n",
       " ',',\n",
       " 'the',\n",
       " 'new',\n",
       " 'discovery',\n",
       " 'of',\n",
       " 'use',\n",
       " 'of',\n",
       " 'graphical',\n",
       " 'processing',\n",
       " 'units',\n",
       " '(',\n",
       " 'GPU',\n",
       " ')',\n",
       " 'improved',\n",
       " 'digital',\n",
       " 'neural',\n",
       " 'networks',\n",
       " 'and',\n",
       " 'NLP',\n",
       " '.',\n",
       " 'NLP',\n",
       " 'empowers',\n",
       " 'computer',\n",
       " 'programs',\n",
       " 'to',\n",
       " 'comprehend',\n",
       " 'unstructured',\n",
       " 'content',\n",
       " 'by',\n",
       " 'utilizing',\n",
       " 'AI',\n",
       " 'and',\n",
       " 'machine',\n",
       " 'learning',\n",
       " 'to',\n",
       " 'make',\n",
       " 'derivations',\n",
       " 'and',\n",
       " 'give',\n",
       " 'context',\n",
       " 'to',\n",
       " 'language',\n",
       " ',',\n",
       " 'similarly',\n",
       " 'as',\n",
       " 'human',\n",
       " 'brains',\n",
       " 'do',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'a',\n",
       " 'device',\n",
       " 'for',\n",
       " 'revealing',\n",
       " 'and',\n",
       " 'analysing',\n",
       " 'the',\n",
       " '“',\n",
       " 'signals',\n",
       " '”',\n",
       " 'covered',\n",
       " 'in',\n",
       " 'unstructured',\n",
       " 'information',\n",
       " '.',\n",
       " 'Organizations',\n",
       " 'would',\n",
       " 'then',\n",
       " 'be',\n",
       " 'able',\n",
       " 'to',\n",
       " 'get',\n",
       " 'a',\n",
       " 'deeper',\n",
       " 'comprehension',\n",
       " 'of',\n",
       " 'public',\n",
       " 'perception',\n",
       " 'around',\n",
       " 'their',\n",
       " 'products',\n",
       " ',',\n",
       " 'services',\n",
       " 'and',\n",
       " 'brand',\n",
       " ',',\n",
       " 'just',\n",
       " 'as',\n",
       " 'those',\n",
       " 'of',\n",
       " 'their',\n",
       " 'rivals',\n",
       " '.',\n",
       " 'Now',\n",
       " 'Google',\n",
       " 'has',\n",
       " 'released',\n",
       " 'its',\n",
       " 'own',\n",
       " 'neural-net-based',\n",
       " 'engine',\n",
       " 'for',\n",
       " 'eight',\n",
       " 'language',\n",
       " 'pairs',\n",
       " ',',\n",
       " 'closing',\n",
       " 'much',\n",
       " 'of',\n",
       " 'the',\n",
       " 'quality',\n",
       " 'gap',\n",
       " 'between',\n",
       " 'its',\n",
       " 'old',\n",
       " 'system',\n",
       " 'and',\n",
       " 'a',\n",
       " 'human',\n",
       " 'translator',\n",
       " 'and',\n",
       " 'fuelling',\n",
       " 'increasing',\n",
       " 'interest',\n",
       " 'in',\n",
       " 'the',\n",
       " 'technology',\n",
       " '.',\n",
       " 'Computers',\n",
       " 'today',\n",
       " 'can',\n",
       " 'already',\n",
       " 'produce',\n",
       " 'an',\n",
       " 'eerie',\n",
       " 'echo',\n",
       " 'of',\n",
       " 'human',\n",
       " 'language',\n",
       " 'if',\n",
       " 'fed',\n",
       " 'with',\n",
       " 'the',\n",
       " 'appropriate',\n",
       " 'material',\n",
       " '.',\n",
       " 'Over',\n",
       " 'the',\n",
       " 'past',\n",
       " 'few',\n",
       " 'years',\n",
       " ',',\n",
       " 'Deep',\n",
       " 'Learning',\n",
       " '(',\n",
       " 'DL',\n",
       " ')',\n",
       " 'architectures',\n",
       " 'and',\n",
       " 'algorithms',\n",
       " 'have',\n",
       " 'made',\n",
       " 'impressive',\n",
       " 'advances',\n",
       " 'in',\n",
       " 'fields',\n",
       " 'such',\n",
       " 'as',\n",
       " 'image',\n",
       " 'recognition',\n",
       " 'and',\n",
       " 'speech',\n",
       " 'processing',\n",
       " '.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3462dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "par2=\"\"\"Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider.\n",
    "However, we still can have problems if we only split by space to achieve the wanted results. Some English compound nouns are variably written and sometimes they contain a space. In most cases, we use a library to achieve the wanted results, so again don’t worry too much for the details.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee820b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0965ec06",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=Tokenizer()\n",
    "text=\"Y X X X Y \"\n",
    "t.fit_on_texts(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8259040f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq=t.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57ec4ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': 1, 'y': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77e4e18e",
   "metadata": {},
   "outputs": [],
   "source": [
    "par2=\"\"\"Word tokenization (also called word segmentation) is the problem of dividing a string of written language into its component words. In English and many other languages using some form of Latin alphabet, space is a good approximation of a word divider.\n",
    "However, we still can have problems if we only split by space to achieve the wanted results. Some English compound nouns are variably written and sometimes they contain a space. In most cases, we use a library to achieve the wanted results, so again don’t worry too much for the details.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b2fe7e",
   "metadata": {},
   "source": [
    "## STEMMING THE PAR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6c6a2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "sen=nltk.sent_tokenize(par2)\n",
    "stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca0ee259",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sen)):\n",
    "    words=nltk.word_tokenize(sen[i])\n",
    "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sen[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "418df209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function list.count(value, /)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sen.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9291fc",
   "metadata": {},
   "source": [
    "# LEMMITIZING THE PAR2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9460bf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence=nltk.sent_tokenize(par2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26e4ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "lematizer=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da44c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentence)):\n",
    "    words=nltk.word_tokenize(sentence[i])\n",
    "    words=[lematizer.lemmatize(word) for word in words if word not in set(stopwords.words('english'))]\n",
    "    sentence[i]=' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "27f0f1fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Word tokenization ( also called word segmentation ) problem dividing string written language component word .',\n",
       " 'In English many language using form Latin alphabet , space good approximation word divider .',\n",
       " 'However , still problem split space achieve wanted result .',\n",
       " 'Some English compound noun variably written sometimes contain space .',\n",
       " 'In case , use library achieve wanted result , ’ worry much detail .']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc4f74b",
   "metadata": {},
   "source": [
    "# CLEANING THE TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d28822cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "74b2e133",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps=PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e70c95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b84e603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentence)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',sentence[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[ps.stem(word) for word in review if not  word in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    corpus.append(review)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8d49f1",
   "metadata": {},
   "source": [
    "## CREATING BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b88f1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "cv=CountVectorizer(max_features=1500) # COUNTVECTORIZER IS RESPONSIBLE FOR FOR CREATING BAG OF WORD\n",
    "X=cv.fit_transform(corpus).toarray()  # FIT_TRANSFORM IS RESPONSIBLE FOR CREATING MATRIX LIKE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be1d079b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0 0 3 0 1]\n",
      " [0 1 0 1 0 0 0 0 0 0 1 1 1 1 0 1 1 0 1 0 0 0 0 0 0 1 0 0 0 0 1 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 0 0 1 1 1 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 1]\n",
      " [1 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "246d69b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "174e7092",
   "metadata": {},
   "outputs": [],
   "source": [
    "messege=pd.read_csv('SpamMessege' ,sep='\\t',names=[\"label\",\"messege\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f15769e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>messege</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will ü b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                            messege\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham               Will ü b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messege"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014445a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, there my name is Aneka. I will answer your queries. If you want to exit, type Bye!\n",
      "hello\n",
      "Aneka:hey\n",
      "hi\n",
      "Aneka:hello\n",
      "hey\n",
      "Aneka:I am glad! you are talking to me\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "f = open('chatbot.txt', 'r', errors='ignore')\n",
    "raw = f.read()\n",
    "raw = raw.lower()\n",
    "\n",
    "sent_tokens = nltk.sent_tokenize(raw) #converts to list of scentences\n",
    "word_tokens = nltk.word_tokenize(raw) #converts to list of words\n",
    "\n",
    "sentToken = sent_tokens[:4]\n",
    "#print(sentToken)\n",
    "wordToken = word_tokens[:4]\n",
    "#print(wordToken)\n",
    "\n",
    "#preprocessing \n",
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "remove_punct_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))\n",
    "\n",
    "#Greetings\n",
    "GREETING_INPUTS = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\")\n",
    "GREETING_RESPONSES = [\"hi\", \"hey\", \"nods\", \"hi there\", \"hello\", \"I am glad! you are talking to me\"]\n",
    "\n",
    "def greeting(scentence):\n",
    "    \n",
    "    for word in scentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)\n",
    "        \n",
    "#Vectorizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def response(user_response):\n",
    "    chatbot_response = ''\n",
    "    sent_tokens.append(user_response)\n",
    "    TfidfVec = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\")\n",
    "    tfidf = TfidfVec.fit_transform(sent_tokens)\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    flat = vals.flatten()\n",
    "    flat.sort()\n",
    "    req_tfidf = flat[-2]\n",
    "    if(req_tfidf == 0):\n",
    "        chatbot_response = chatbot_response + \"I am sorry! I don't understand you\"\n",
    "        return chatbot_response\n",
    "    \n",
    "    else:\n",
    "        chatbot_response=chatbot_response+sent_tokens[idx]\n",
    "        return chatbot_response\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    flag = True\n",
    "    print(\"Hello, there my name is Aneka. I will answer your queries. If you want to exit, type Bye!\")\n",
    "    while(flag==True):\n",
    "        user_response = input()\n",
    "        user_response = user_response.lower()\n",
    "        if(user_response!='bye'):\n",
    "            if user_response == 'thanks' or user_response == 'thank you':\n",
    "                flag = False\n",
    "                print(\"Aneka: You're welcome!\")\n",
    "            else:\n",
    "                if(greeting(user_response)!=None):\n",
    "                    print(\"Aneka:\" +greeting(user_response))\n",
    "                else:\n",
    "                    print(\"Aneka:\", end='')\n",
    "                    print(response(user_response))\n",
    "                    sent_tokens.remove(user_response)\n",
    "        else:\n",
    "            flag = False\n",
    "            print(\"Aneka: Bye! Have a great time!\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb0fb17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
